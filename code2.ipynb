{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s2JFE1r_PDHB"
      },
      "outputs": [],
      "source": [
        "import sys, time, argparse\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tensorflow.contrib.layers import l2_regularizer\n",
        "from tensorflow.contrib.layers import batch_norm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_VALIDATION_RATIO = 0.1"
      ],
      "metadata": {
        "id": "qxiswcdoPF6_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Medgan(object):\n",
        "    def __init__(self,\n",
        "                 dataType='binary',\n",
        "                 inputDim=615,\n",
        "                 embeddingDim=128,\n",
        "                 randomDim=128,\n",
        "                 generatorDims=(128, 128),\n",
        "                 discriminatorDims=(256, 128, 1),\n",
        "                 compressDims=(),\n",
        "                 decompressDims=(),\n",
        "                 bnDecay=0.99,\n",
        "                 l2scale=0.001):\n",
        "        self.inputDim = inputDim\n",
        "        self.embeddingDim = embeddingDim\n",
        "        self.generatorDims = list(generatorDims) + [embeddingDim]\n",
        "        self.randomDim = randomDim\n",
        "        self.dataType = dataType\n",
        "\n",
        "        if dataType == 'binary':\n",
        "            self.aeActivation = tf.nn.tanh\n",
        "        else:\n",
        "            self.aeActivation = tf.nn.relu\n",
        "\n",
        "        self.generatorActivation = tf.nn.relu\n",
        "        self.discriminatorActivation = tf.nn.relu\n",
        "        self.discriminatorDims = discriminatorDims\n",
        "        self.compressDims = list(compressDims) + [embeddingDim]\n",
        "        self.decompressDims = list(decompressDims) + [inputDim]\n",
        "        self.bnDecay = bnDecay\n",
        "        self.l2scale = l2scale"
      ],
      "metadata": {
        "id": "OeKae2f6P5Cx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadData(self, dataPath=''):\n",
        "        data = np.load(dataPath, allow_pickle=True)\n",
        "\n",
        "        if self.dataType == 'binary':\n",
        "            data = np.clip(data, 0, 1)\n",
        "\n",
        "        trainX, validX = train_test_split(data, test_size=_VALIDATION_RATIO, random_state=0)\n",
        "        return trainX, "
      ],
      "metadata": {
        "id": "ow0kSKLPQBgl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def buildAutoencoder(self, x_input):\n",
        "        decodeVariables = {}\n",
        "        with tf.variable_scope('autoencoder', regularizer=l2_regularizer(self.l2scale)):\n",
        "            tempVec = x_input\n",
        "            tempDim = self.inputDim\n",
        "            i = 0\n",
        "            for compressDim in self.compressDims:\n",
        "                W = tf.get_variable('aee_W_'+str(i), shape=[tempDim, compressDim])\n",
        "                b = tf.get_variable('aee_b_'+str(i), shape=[compressDim])\n",
        "                tempVec = self.aeActivation(tf.add(tf.matmul(tempVec, W), b))\n",
        "                tempDim = compressDim\n",
        "                i += 1"
      ],
      "metadata": {
        "id": "aEHORigqQFpO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " i = 0\n",
        "            for decompressDim in self.decompressDims[:-1]:\n",
        "                W = tf.get_variable('aed_W_'+str(i), shape=[tempDim, decompressDim])\n",
        "                b = tf.get_variable('aed_b_'+str(i), shape=[decompressDim])\n",
        "                tempVec = self.aeActivation(tf.add(tf.matmul(tempVec, W), b))\n",
        "                tempDim = decompressDim\n",
        "                decodeVariables['aed_W_'+str(i)] = W\n",
        "                decodeVariables['aed_b_'+str(i)] = b\n",
        "                i += 1\n",
        "            W = tf.get_variable('aed_W_'+str(i), shape=[tempDim, self.decompressDims[-1]])\n",
        "            b = tf.get_variable('aed_b_'+str(i), shape=[self.decompressDims[-1]])\n",
        "            decodeVariables['aed_W_'+str(i)] = W\n",
        "            decodeVariables['aed_b_'+str(i)] = b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "Ggdnh4GpQLVV",
        "outputId": "789be0f8-bca1-416a-9a8b-586861bffae7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-c0bf338409db>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    for decompressDim in self.decompressDims[:-1]:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "            if self.dataType == 'binary':\n",
        "                x_reconst = tf.nn.sigmoid(tf.add(tf.matmul(tempVec,W),b))\n",
        "                loss = tf.reduce_mean(-tf.reduce_sum(x_input * tf.log(x_reconst + 1e-12) + (1. - x_input) * tf.log(1. - x_reconst + 1e-12), 1), 0)\n",
        "            else:\n",
        "                x_reconst = tf.nn.relu(tf.add(tf.matmul(tempVec,W),b))\n",
        "                loss = tf.reduce_mean((x_input - x_reconst)**2)\n",
        "            \n",
        "        return loss, decodeVariables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "br79l16BQYUJ",
        "outputId": "00ab11ff-f660-4fea-cd4f-a6b4d6fbad3b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-e91386adb277>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    return loss, decodeVariables\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def buildGenerator(self, x_input, bn_train):\n",
        "        tempVec = x_input\n",
        "        tempDim = self.randomDim\n",
        "        with tf.variable_scope('generator', regularizer=l2_regularizer(self.l2scale)):\n",
        "            for i, genDim in enumerate(self.generatorDims[:-1]):\n",
        "                W = tf.get_variable('W_'+str(i), shape=[tempDim, genDim])\n",
        "                h = tf.matmul(tempVec,W)\n",
        "                h2 = batch_norm(h, decay=self.bnDecay, scale=True, is_training=bn_train, updates_collections=None)\n",
        "                h3 = self.generatorActivation(h2)\n",
        "                tempVec = h3 + tempVec\n",
        "                tempDim = genDim\n",
        "            W = tf.get_variable('W'+str(i), shape=[tempDim, self.generatorDims[-1]])\n",
        "            h = tf.matmul(tempVec,W)\n",
        "            h2 = batch_norm(h, decay=self.bnDecay, scale=True, is_training=bn_train, updates_collections=None)\n",
        "\n",
        "            if self.dataType == 'binary':\n",
        "                h3 = tf.nn.tanh(h2)\n",
        "            else:\n",
        "                h3 = tf.nn.relu(h2)\n",
        "\n",
        "            output = h3 + tempVec\n",
        "        return output"
      ],
      "metadata": {
        "id": "yw4lq5R8QhAb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def buildGeneratorTest(self, x_input, bn_train):\n",
        "        tempVec = x_input\n",
        "        tempDim = self.randomDim\n",
        "        with tf.variable_scope('generator', regularizer=l2_regularizer(self.l2scale)):\n",
        "            for i, genDim in enumerate(self.generatorDims[:-1]):\n",
        "                W = tf.get_variable('W_'+str(i), shape=[tempDim, genDim])\n",
        "                h = tf.matmul(tempVec,W)\n",
        "                h2 = batch_norm(h, decay=self.bnDecay, scale=True, is_training=bn_train, updates_collections=None, trainable=False)\n",
        "                h3 = self.generatorActivation(h2)\n",
        "                tempVec = h3 + tempVec\n",
        "                tempDim = genDim\n",
        "            W = tf.get_variable('W'+str(i), shape=[tempDim, self.generatorDims[-1]])\n",
        "            h = tf.matmul(tempVec,W)\n",
        "            h2 = batch_norm(h, decay=self.bnDecay, scale=True, is_training=bn_train, updates_collections=None, trainable=False)\n",
        "\n",
        "            if self.dataType == 'binary':\n",
        "                h3 = tf.nn.tanh(h2)\n",
        "            else:\n",
        "                h3 = tf.nn.relu(h2)\n",
        "\n",
        "            output = h3 + tempVec\n",
        "        return output"
      ],
      "metadata": {
        "id": "1JDVLE1NQwxF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getDiscriminatorResults(self, x_input, keepRate, reuse=False):\n",
        "        batchSize = tf.shape(x_input)[0]\n",
        "        inputMean = tf.reshape(tf.tile(tf.reduce_mean(x_input,0), [batchSize]), (batchSize, self.inputDim))\n",
        "        tempVec = tf.concat([x_input, inputMean], 1)\n",
        "        tempDim = self.inputDim * 2\n",
        "        with tf.variable_scope('discriminator', reuse=reuse, regularizer=l2_regularizer(self.l2scale)):\n",
        "            for i, discDim in enumerate(self.discriminatorDims[:-1]):\n",
        "                W = tf.get_variable('W_'+str(i), shape=[tempDim, discDim])\n",
        "                b = tf.get_variable('b_'+str(i), shape=[discDim])\n",
        "                h = self.discriminatorActivation(tf.add(tf.matmul(tempVec,W),b))\n",
        "                h = tf.nn.dropout(h, keepRate)\n",
        "                tempVec = h\n",
        "                tempDim = discDim\n",
        "            W = tf.get_variable('W', shape=[tempDim, 1])\n",
        "            b = tf.get_variable('b', shape=[1])\n",
        "            y_hat = tf.squeeze(tf.nn.sigmoid(tf.add(tf.matmul(tempVec, W), b)))\n",
        "        return y_hat"
      ],
      "metadata": {
        "id": "5ASp1m10Q6uI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def buildDiscriminator(self, x_real, x_fake, keepRate, decodeVariables, bn_train):\n",
        "        #Discriminate for real samples\n",
        "        y_hat_real = self.getDiscriminatorResults(x_real, keepRate, reuse=False)\n",
        "\n",
        "        #Decompress, then discriminate for real samples\n",
        "        tempVec = x_fake\n",
        "        i = 0\n",
        "        for _ in self.decompressDims[:-1]:\n",
        "            tempVec = self.aeActivation(tf.add(tf.matmul(tempVec, decodeVariables['aed_W_'+str(i)]), decodeVariables['aed_b_'+str(i)]))\n",
        "            i += 1\n",
        "\n",
        "        if self.dataType == 'binary':\n",
        "            x_decoded = tf.nn.sigmoid(tf.add(tf.matmul(tempVec, decodeVariables['aed_W_'+str(i)]), decodeVariables['aed_b_'+str(i)]))\n",
        "        else:\n",
        "            x_decoded = tf.nn.relu(tf.add(tf.matmul(tempVec, decodeVariables['aed_W_'+str(i)]), decodeVariables['aed_b_'+str(i)]))\n",
        "\n",
        "        y_hat_fake = self.getDiscriminatorResults(x_decoded, keepRate, reuse=True)\n",
        "\n",
        "        loss_d = -tf.reduce_mean(tf.log(y_hat_real + 1e-12)) - tf.reduce_mean(tf.log(1. - y_hat_fake + 1e-12))\n",
        "        loss_g = -tf.reduce_mean(tf.log(y_hat_fake + 1e-12))\n",
        "\n",
        "        return loss_d, loss_g, y_hat_real, y_hat_fake"
      ],
      "metadata": {
        "id": "qW2rxkagQ861"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print2file(self, buf, outFile):\n",
        "        outfd = open(outFile, 'a')\n",
        "        outfd.write(buf + '\\n')\n",
        "        outfd.close()"
      ],
      "metadata": {
        "id": "_e7JoY_WRD6o"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generateData(self,\n",
        "                     nSamples=100,\n",
        "                     modelFile='model',\n",
        "                     batchSize=100,\n",
        "                     outFile='out'):\n",
        "        x_dummy = tf.placeholder('float', [None, self.inputDim])\n",
        "        _, decodeVariables = self.buildAutoencoder(x_dummy)\n",
        "        x_random = tf.placeholder('float', [None, self.randomDim])\n",
        "        bn_train = tf.placeholder('bool')\n",
        "        x_emb = self.buildGeneratorTest(x_random, bn_train)\n",
        "        tempVec = x_emb\n",
        "        i = 0\n",
        "        for _ in self.decompressDims[:-1]:\n",
        "            tempVec = self.aeActivation(tf.add(tf.matmul(tempVec, decodeVariables['aed_W_'+str(i)]), decodeVariables['aed_b_'+str(i)]))\n",
        "            i += 1\n",
        "\n",
        "        if self.dataType == 'binary':\n",
        "            x_reconst = tf.nn.sigmoid(tf.add(tf.matmul(tempVec, decodeVariables['aed_W_'+str(i)]), decodeVariables['aed_b_'+str(i)]))\n",
        "        else:\n",
        "            x_reconst = tf.nn.relu(tf.add(tf.matmul(tempVec, decodeVariables['aed_W_'+str(i)]), decodeVariables['aed_b_'+str(i)]))\n",
        "\n",
        "        np.random.seed(1234)\n",
        "        saver = tf.train.Saver()\n",
        "        outputVec = []\n",
        "        burn_in = 1000\n",
        "        with tf.Session() as sess:\n",
        "            saver.restore(sess, modelFile)\n",
        "            print('burning in')\n",
        "            for i in range(burn_in):\n",
        "                randomX = np.random.normal(size=(batchSize, self.randomDim))\n",
        "                output = sess.run(x_reconst, feed_dict={x_random:randomX, bn_train:True})\n",
        "\n",
        "            print('generating')\n",
        "            nBatches = int(np.ceil(float(nSamples)) / float(batchSize))\n",
        "            for i in range(nBatches):\n",
        "                randomX = np.random.normal(size=(batchSize, self.randomDim))\n",
        "                output = sess.run(x_reconst, feed_dict={x_random:randomX, bn_train:False})\n",
        "                outputVec.extend(output)\n",
        "\n",
        "        outputMat = np.array(outputVec)\n",
        "        np.save(outFile, outputMat)"
      ],
      "metadata": {
        "id": "nmN-yMEPRNv-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def calculateDiscAuc(self, preds_real, preds_fake):\n",
        "        preds = np.concatenate([preds_real, preds_fake], axis=0)\n",
        "        labels = np.concatenate([np.ones((len(preds_real))), np.zeros((len(preds_fake)))], axis=0)\n",
        "        auc = roc_auc_score(labels, preds)\n",
        "        return auc"
      ],
      "metadata": {
        "id": "0du8F0LWREJN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def calculateDiscAccuracy(self, preds_real, preds_fake):\n",
        "        total = len(preds_real) + len(preds_fake)\n",
        "        hit = 0\n",
        "        for pred in preds_real: \n",
        "            if pred > 0.5: hit += 1\n",
        "        for pred in preds_fake: \n",
        "            if pred < 0.5: hit += 1\n",
        "        acc = float(hit) / float(total)\n",
        "        return acc\n",
        "        "
      ],
      "metadata": {
        "id": "ZHXXRQkrRTKb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def train(self,\n",
        "              dataPath='data',\n",
        "              modelPath='',\n",
        "              outPath='out',\n",
        "              nEpochs=500,\n",
        "              discriminatorTrainPeriod=2,\n",
        "              generatorTrainPeriod=1,\n",
        "              pretrainBatchSize=100,\n",
        "              batchSize=1000,\n",
        "              pretrainEpochs=100,\n",
        "              saveMaxKeep=0):\n",
        "        x_raw = tf.placeholder('float', [None, self.inputDim])\n",
        "        x_random= tf.placeholder('float', [None, self.randomDim])\n",
        "        keep_prob = tf.placeholder('float')\n",
        "        bn_train = tf.placeholder('bool')\n",
        "\n",
        "        loss_ae, decodeVariables = self.buildAutoencoder(x_raw)\n",
        "        x_fake = self.buildGenerator(x_random, bn_train)\n",
        "        loss_d, loss_g, y_hat_real, y_hat_fake = self.buildDiscriminator(x_raw, x_fake, keep_prob, decodeVariables, bn_train)\n",
        "        trainX, validX = self.loadData(dataPath)\n",
        "\n",
        "        t_vars = tf.trainable_variables()\n",
        "        ae_vars = [var for var in t_vars if 'autoencoder' in var.name]\n",
        "        d_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
        "        g_vars = [var for var in t_vars if 'generator' in var.name]\n",
        "\n",
        "        all_regs = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "\n",
        "        optimize_ae = tf.train.AdamOptimizer().minimize(loss_ae + sum(all_regs), var_list=ae_vars)\n",
        "        optimize_d = tf.train.AdamOptimizer().minimize(loss_d + sum(all_regs), var_list=d_vars)\n",
        "        decodeVariablesValues = list(decodeVariables.values())\n",
        "        optimize_g = tf.train.AdamOptimizer().minimize(loss_g + sum(all_regs), var_list=g_vars+decodeVariablesValues)\n",
        "\n",
        "        initOp = tf.global_variables_initializer()\n",
        "\n",
        "        nBatches = int(np.ceil(float(trainX.shape[0]) / float(batchSize)))\n",
        "        saver = tf.train.Saver(max_to_keep=saveMaxKeep)\n",
        "        logFile = outPath + '.log'\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "            if modelPath == '': sess.run(initOp)\n",
        "            else: saver.restore(sess, modelPath)\n",
        "            nTrainBatches = int(np.ceil(float(trainX.shape[0])) / float(pretrainBatchSize))\n",
        "            nValidBatches = int(np.ceil(float(validX.shape[0])) / float(pretrainBatchSize))\n",
        "\n",
        "            if modelPath== '':\n",
        "                for epoch in range(pretrainEpochs):\n",
        "                    idx = np.random.permutation(trainX.shape[0])\n",
        "                    trainLossVec = []\n",
        "                    for i in range(nTrainBatches):\n",
        "                        batchX = trainX[idx[i*pretrainBatchSize:(i+1)*pretrainBatchSize]]\n",
        "                        _, loss = sess.run([optimize_ae, loss_ae], feed_dict={x_raw:batchX})\n",
        "                        trainLossVec.append(loss)\n",
        "                    idx = np.random.permutation(validX.shape[0])\n",
        "                    validLossVec = []\n",
        "                    for i in range(nValidBatches):\n",
        "                        batchX = validX[idx[i*pretrainBatchSize:(i+1)*pretrainBatchSize]]\n",
        "                        loss = sess.run(loss_ae, feed_dict={x_raw:batchX})\n",
        "                        validLossVec.append(loss)\n",
        "                    validReverseLoss = 0.\n",
        "                    buf = 'Pretrain_Epoch:%d, trainLoss:%f, validLoss:%f, validReverseLoss:%f' % (epoch, np.mean(trainLossVec), np.mean(validLossVec), validReverseLoss)\n",
        "                    print(buf)\n",
        "                    self.print2file(buf, logFile)\n",
        "\n",
        "            idx = np.arange(trainX.shape[0])\n",
        "            for epoch in range(nEpochs):\n",
        "                d_loss_vec= []\n",
        "                g_loss_vec = []\n",
        "                for i in range(nBatches):\n",
        "                    for _ in range(discriminatorTrainPeriod):\n",
        "                        batchIdx = np.random.choice(idx, size=batchSize, replace=False)\n",
        "                        batchX = trainX[batchIdx]\n",
        "                        randomX = np.random.normal(size=(batchSize, self.randomDim))\n",
        "                        _, discLoss = sess.run([optimize_d, loss_d], feed_dict={x_raw:batchX, x_random:randomX, keep_prob:1.0, bn_train:False})\n",
        "                        d_loss_vec.append(discLoss)\n",
        "                    for _ in range(generatorTrainPeriod):\n",
        "                        randomX = np.random.normal(size=(batchSize, self.randomDim))\n",
        "                        _, generatorLoss = sess.run([optimize_g, loss_g], feed_dict={x_raw:batchX, x_random:randomX, keep_prob:1.0, bn_train:True})\n",
        "                        g_loss_vec.append(generatorLoss)\n",
        "\n",
        "                idx = np.arange(len(validX))\n",
        "                nValidBatches = int(np.ceil(float(len(validX)) / float(batchSize)))\n",
        "                validAccVec = []\n",
        "                validAucVec = []\n",
        "                for i in range(nBatches):\n",
        "                    batchIdx = np.random.choice(idx, size=batchSize, replace=False)\n",
        "                    batchX = validX[batchIdx]\n",
        "                    randomX = np.random.normal(size=(batchSize, self.randomDim))\n",
        "                    preds_real, preds_fake, = sess.run([y_hat_real, y_hat_fake], feed_dict={x_raw:batchX, x_random:randomX, keep_prob:1.0, bn_train:False})\n",
        "                    validAcc = self.calculateDiscAccuracy(preds_real, preds_fake)\n",
        "                    validAuc = self.calculateDiscAuc(preds_real, preds_fake)\n",
        "                    validAccVec.append(validAcc)\n",
        "                    validAucVec.append(validAuc)\n",
        "                buf = 'Epoch:%d, d_loss:%f, g_loss:%f, accuracy:%f, AUC:%f' % (epoch, np.mean(d_loss_vec), np.mean(g_loss_vec), np.mean(validAccVec), np.mean(validAucVec))\n",
        "                print(buf)\n",
        "                self.print2file(buf, logFile)\n",
        "                savePath = saver.save(sess, outPath, global_step=epoch)\n",
        "        print(savePath)"
      ],
      "metadata": {
        "id": "RW53lpF0RXe9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def str2bool(v):\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Boolean value expected.')"
      ],
      "metadata": {
        "id": "b5649YofRhU8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_arguments(parser):\n",
        "    parser.add_argument('--embed_size', type=int, default=128, help='The dimension size of the embedding, which will be generated by the generator. (default value: 128)')\n",
        "    parser.add_argument('--noise_size', type=int, default=128, help='The dimension size of the random noise, on which the generator is conditioned. (default value: 128)')\n",
        "    parser.add_argument('--generator_size', type=tuple, default=(128, 128), help='The dimension size of the generator. Note that another layer of size \"--embed_size\" is always added. (default value: (128, 128))')\n",
        "    parser.add_argument('--discriminator_size', type=tuple, default=(256, 128, 1), help='The dimension size of the discriminator. (default value: (256, 128, 1))')\n",
        "    parser.add_argument('--compressor_size', type=tuple, default=(), help='The dimension size of the encoder of the autoencoder. Note that another layer of size \"--embed_size\" is always added. Therefore this can be a blank tuple. (default value: ())')\n",
        "    parser.add_argument('--decompressor_size', type=tuple, default=(), help='The dimension size of the decoder of the autoencoder. Note that another layer, whose size is equal to the dimension of the <patient_matrix>, is always added. Therefore this can be a blank tuple. (default value: ())')\n",
        "    parser.add_argument('--data_type', type=str, default='binary', choices=['binary', 'count'], help='The input data type. The <patient matrix> could either contain binary values or count values. (default value: \"binary\")')\n",
        "    parser.add_argument('--batchnorm_decay', type=float, default=0.99, help='Decay value for the moving average used in Batch Normalization. (default value: 0.99)')\n",
        "    parser.add_argument('--L2', type=float, default=0.001, help='L2 regularization coefficient for all weights. (default value: 0.001)')\n",
        "\n",
        "    parser.add_argument('data_file', type=str, metavar='<patient_matrix>', help='The path to the numpy matrix containing aggregated patient records.')\n",
        "    parser.add_argument('out_file', type=str, metavar='<out_file>', help='The path to the output models.')\n",
        "    parser.add_argument('--model_file', type=str, metavar='<model_file>', default='', help='The path to the model file, in case you want to continue training. (default value: '')')\n",
        "    parser.add_argument('--n_pretrain_epoch', type=int, default=100, help='The number of epochs to pre-train the autoencoder. (default value: 100)')\n",
        "    parser.add_argument('--n_epoch', type=int, default=1000, help='The number of epochs to train medGAN. (default value: 1000)')\n",
        "    parser.add_argument('--n_discriminator_update', type=int, default=2, help='The number of times to update the discriminator per epoch. (default value: 2)')\n",
        "    parser.add_argument('--n_generator_update', type=int, default=1, help='The number of times to update the generator per epoch. (default value: 1)')\n",
        "    parser.add_argument('--pretrain_batch_size', type=int, default=100, help='The size of a single mini-batch for pre-training the autoencoder. (default value: 100)')\n",
        "    parser.add_argument('--batch_size', type=int, default=1000, help='The size of a single mini-batch for training medGAN. (default value: 1000)')\n",
        "    parser.add_argument('--save_max_keep', type=int, default=0, help='The number of models to keep. Setting this to 0 will save models for every epoch. (default value: 0)')\n",
        "    parser.add_argument('--generate_data', type=str2bool, default=False, help='If True the model generates data, if False the model is trained (default value: False)')\n",
        "    args = parser.parse_args()\n",
        "    return args\n"
      ],
      "metadata": {
        "id": "HDTpCwOBRXjO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    args = parse_arguments(parser)\n",
        "\n",
        "    data = np.load(args.data_file, allow_pickle=True)\n",
        "    inputDim = data.shape[1]\n",
        "\n",
        "    mg = Medgan(dataType=args.data_type,\n",
        "                inputDim=inputDim,\n",
        "                embeddingDim=args.embed_size,\n",
        "                randomDim=args.noise_size,\n",
        "                generatorDims=args.generator_size,\n",
        "                discriminatorDims=args.discriminator_size,\n",
        "                compressDims=args.compressor_size,\n",
        "                decompressDims=args.decompressor_size,\n",
        "                bnDecay=args.batchnorm_decay,\n",
        "                l2scale=args.L2)\n",
        "\n",
        "    # True for generation, False for training\n",
        "    if not args.generate_data:\n",
        "    # Training\n",
        "        mg.train(dataPath=args.data_file,\n",
        "                 modelPath=args.model_file,\n",
        "                 outPath=args.out_file,\n",
        "                 pretrainEpochs=args.n_pretrain_epoch,\n",
        "                 nEpochs=args.n_epoch,\n",
        "                 discriminatorTrainPeriod=args.n_discriminator_update,\n",
        "                 generatorTrainPeriod=args.n_generator_update,\n",
        "                 pretrainBatchSize=args.pretrain_batch_size,\n",
        "                 batchSize=args.batch_size,\n",
        "                 saveMaxKeep=args.save_max_keep)\n",
        "    else:\n",
        "    # Generate synthetic data using a trained model\n",
        "    # You must specify \"--model_file\" and \"<out_file>\" to generate synthetic data.\n",
        "        mg.generateData(nSamples=10000,\n",
        "                        modelFile=args.model_file,\n",
        "                        batchSize=args.batch_size,\n",
        "                        outFile=args.out_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "l86oSbr5RXrR",
        "outputId": "dd07a5f2-6f2c-498b-b7b6-b291de7f7c20"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--embed_size EMBED_SIZE]\n",
            "                             [--noise_size NOISE_SIZE]\n",
            "                             [--generator_size GENERATOR_SIZE]\n",
            "                             [--discriminator_size DISCRIMINATOR_SIZE]\n",
            "                             [--compressor_size COMPRESSOR_SIZE]\n",
            "                             [--decompressor_size DECOMPRESSOR_SIZE]\n",
            "                             [--data_type {binary,count}]\n",
            "                             [--batchnorm_decay BATCHNORM_DECAY] [--L2 L2]\n",
            "                             [--model_file <model_file>]\n",
            "                             [--n_pretrain_epoch N_PRETRAIN_EPOCH]\n",
            "                             [--n_epoch N_EPOCH]\n",
            "                             [--n_discriminator_update N_DISCRIMINATOR_UPDATE]\n",
            "                             [--n_generator_update N_GENERATOR_UPDATE]\n",
            "                             [--pretrain_batch_size PRETRAIN_BATCH_SIZE]\n",
            "                             [--batch_size BATCH_SIZE]\n",
            "                             [--save_max_keep SAVE_MAX_KEEP]\n",
            "                             [--generate_data GENERATE_DATA]\n",
            "                             <patient_matrix> <out_file>\n",
            "ipykernel_launcher.py: error: the following arguments are required: <out_file>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    }
  ]
}